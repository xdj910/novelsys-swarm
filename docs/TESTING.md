# æµ‹è¯•æŒ‡å—

> NOVELSYS-SWARM 2.5 æµ‹è¯•ç­–ç•¥å’Œæ‰§è¡ŒæŒ‡å—  
> Version: 2.5.0 | Updated: 2025-01-30

## ğŸ“‹ ç›®å½•

- [æµ‹è¯•æ¶æ„](#æµ‹è¯•æ¶æ„)
- [æµ‹è¯•åˆ†ç±»](#æµ‹è¯•åˆ†ç±»)
- [è¿è¡Œæµ‹è¯•](#è¿è¡Œæµ‹è¯•)
- [ç¼–å†™æµ‹è¯•](#ç¼–å†™æµ‹è¯•)
- [æµ‹è¯•è¦†ç›–ç‡](#æµ‹è¯•è¦†ç›–ç‡)
- [æŒç»­é›†æˆ](#æŒç»­é›†æˆ)

## ğŸ—ï¸ æµ‹è¯•æ¶æ„

### æµ‹è¯•å±‚çº§

```
æµ‹è¯•é‡‘å­—å¡”
    â•±â•²
   â•±E2Eâ•²      (5%) - ç«¯åˆ°ç«¯æµ‹è¯•
  â•±â”€â”€â”€â”€â”€â”€â•²
 â•±é›†æˆæµ‹è¯•â•²    (15%) - æ¨¡å—é›†æˆ
â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
â•± å•å…ƒæµ‹è¯• â•²  (80%) - å‡½æ•°çº§æµ‹è¯•
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### æµ‹è¯•ç›®å½•ç»“æ„

```
tests/
â”œâ”€â”€ unit/                 # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_context_firewall.py
â”‚   â”œâ”€â”€ test_github_integration.py
â”‚   â”œâ”€â”€ test_parallel_coordinator.py
â”‚   â””â”€â”€ test_dependency_manager.py
â”œâ”€â”€ integration/         # é›†æˆæµ‹è¯•
â”‚   â””â”€â”€ test_full_workflow.py
â”œâ”€â”€ e2e/                # ç«¯åˆ°ç«¯æµ‹è¯•
â”‚   â””â”€â”€ test_novel_generation.py
â”œâ”€â”€ fixtures/           # æµ‹è¯•å›ºä»¶
â”œâ”€â”€ mocks/             # æ¨¡æ‹Ÿå¯¹è±¡
â””â”€â”€ conftest.py        # pytesté…ç½®
```

## ğŸ“Š æµ‹è¯•åˆ†ç±»

### 1. å•å…ƒæµ‹è¯•

æµ‹è¯•å•ä¸ªå‡½æ•°æˆ–ç±»çš„åŠŸèƒ½ã€‚

**æ ¸å¿ƒæ¨¡å—æµ‹è¯•**:
- âœ… `test_context_firewall.py` - Contexté˜²ç«å¢™æµ‹è¯•
- âœ… `test_github_integration.py` - GitHubé›†æˆæµ‹è¯•
- âœ… `test_parallel_coordinator.py` - å¹¶è¡Œåè°ƒå™¨æµ‹è¯•
- âœ… `test_dependency_manager.py` - ä¾èµ–ç®¡ç†æµ‹è¯•

**æµ‹è¯•å†…å®¹**:
- æ­£å¸¸è·¯å¾„
- è¾¹ç•Œæ¡ä»¶
- é”™è¯¯å¤„ç†
- æ€§èƒ½è¦æ±‚

### 2. é›†æˆæµ‹è¯•

æµ‹è¯•å¤šä¸ªæ¨¡å—çš„åä½œã€‚

**å·¥ä½œæµæµ‹è¯•**:
- âœ… `test_full_workflow.py` - å®Œæ•´å·¥ä½œæµæµ‹è¯•
- 5é˜¶æ®µæµç¨‹æµ‹è¯•
- ä¾èµ–é©±åŠ¨æ‰§è¡Œæµ‹è¯•
- GitHubæŒä¹…åŒ–æµ‹è¯•

### 3. ç«¯åˆ°ç«¯æµ‹è¯•

æµ‹è¯•å®Œæ•´çš„ç”¨æˆ·åœºæ™¯ã€‚

**åœºæ™¯æµ‹è¯•**:
- å°è¯´åˆ›å»ºåˆ°å¯¼å‡º
- æ•…éšœæ¢å¤æµç¨‹
- å¹¶è¡Œç”Ÿæˆæ€§èƒ½

## ğŸš€ è¿è¡Œæµ‹è¯•

### è¿è¡Œæ‰€æœ‰æµ‹è¯•

```bash
# åŸºç¡€è¿è¡Œ
pytest

# è¯¦ç»†è¾“å‡º
pytest -v

# æ˜¾ç¤ºæ‰“å°è¾“å‡º
pytest -s

# å¹¶è¡Œè¿è¡Œ
pytest -n auto
```

### è¿è¡Œç‰¹å®šæµ‹è¯•

```bash
# è¿è¡Œå•ä¸ªæ–‡ä»¶
pytest tests/test_context_firewall.py

# è¿è¡Œå•ä¸ªæµ‹è¯•
pytest tests/test_context_firewall.py::TestContextFirewall::test_filter_agent_response

# è¿è¡ŒæŸç±»æµ‹è¯•
pytest tests/unit/
pytest tests/integration/
```

### è¿è¡Œå¸¦æ ‡è®°çš„æµ‹è¯•

```bash
# åªè¿è¡Œå¿«é€Ÿæµ‹è¯•
pytest -m "not slow"

# åªè¿è¡Œå¼‚æ­¥æµ‹è¯•
pytest -m asyncio

# è·³è¿‡éœ€è¦ç½‘ç»œçš„æµ‹è¯•
pytest -m "not network"
```

### æµ‹è¯•è¦†ç›–ç‡

```bash
# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=src --cov-report=html

# æ˜¾ç¤ºæœªè¦†ç›–çš„è¡Œ
pytest --cov=src --cov-report=term-missing

# è®¾ç½®è¦†ç›–ç‡é˜ˆå€¼
pytest --cov=src --cov-fail-under=80
```

## âœï¸ ç¼–å†™æµ‹è¯•

### æµ‹è¯•å‘½åè§„èŒƒ

**Test file naming specialist:**
â€¢ Use pattern: `test_<module_name>.py`
â€¢ Example: `test_context_firewall.py`

**Test class naming specialist:**
â€¢ Use pattern: `class Test<ClassName>:`
â€¢ Example: `class TestContextFirewall:`

**Test method naming specialist:**
1. Use descriptive pattern: `test_<what_is_being_tested>()`
2. Use scenario pattern: `test_<method>_<scenario>_<expected_result>()`
3. Examples:
   â€¢ `test_filter_agent_response()`
   â€¢ `test_execute_chapter_with_valid_data_returns_result()`

### æµ‹è¯•ç»“æ„ (AAAæ¨¡å¼)

**AAA pattern test structure specialist:**
1. **Arrange (å‡†å¤‡)**: Set up test data and dependencies
   â€¢ Create ContextFirewall instance
   â€¢ Prepare test data dictionary with "content": "test"
   
2. **Act (æ‰§è¡Œ)**: Execute the function being tested
   â€¢ Call firewall.filter_response() with test data
   â€¢ Store the result
   
3. **Assert (æ–­è¨€)**: Verify expected outcomes
   â€¢ Confirm result is not None
   â€¢ Verify "summary" key exists in result

### ä½¿ç”¨Fixtures

**Pytest fixture specialist:**
1. Import pytest module
2. Create coordinator fixture:
   â€¢ Add @pytest.fixture decorator
   â€¢ Define function that creates and returns NovelParallelCoordinator instance
   â€¢ Add docstring: "åˆ›å»ºåè°ƒå™¨å®ä¾‹"

**Fixture-based test specialist:**
1. Create test function accepting coordinator parameter
2. Execute coordinator.execute_chapter() with parameters (1, "outline", {})
3. Assert result is not None
4. Add docstring: "ä½¿ç”¨fixtureçš„æµ‹è¯•"

### å¼‚æ­¥æµ‹è¯•

**Async test specialist:**
1. Import required modules (pytest, asyncio)
2. Create async test function:
   â€¢ Add @pytest.mark.asyncio decorator
   â€¢ Define async function with "test_" prefix
   â€¢ Add docstring: "å¼‚æ­¥å‡½æ•°æµ‹è¯•"
3. Execute async operation:
   â€¢ Await async_function() call
   â€¢ Store result
4. Assert result is not None

### Mockä½¿ç”¨

**Mock testing specialist:**
1. Import Mock and patch from unittest.mock
2. Create mock test function:
   â€¢ Add docstring: "ä½¿ç”¨Mockçš„æµ‹è¯•"
3. Set up patch context:
   â€¢ Use patch('module.function') as mock_func
   â€¢ Set mock_func.return_value to "mocked"
4. Execute test:
   â€¢ Call function_under_test()
   â€¢ Store result
5. Verify behavior:
   â€¢ Assert result equals "expected"
   â€¢ Assert mock_func.assert_called_once()

## ğŸ“ˆ æµ‹è¯•è¦†ç›–ç‡

### å½“å‰è¦†ç›–ç‡ç›®æ ‡

| æ¨¡å— | ç›®æ ‡ | å½“å‰ | çŠ¶æ€ |
|-----|------|------|------|
| core/context_firewall | 90% | 95% | âœ… |
| core/github_integration | 85% | 88% | âœ… |
| core/parallel_coordinator | 85% | 87% | âœ… |
| core/dependency_manager | 90% | 92% | âœ… |
| streams/* | 80% | 75% | âš ï¸ |
| agents/* | 75% | 70% | âš ï¸ |
| **æ€»ä½“** | **80%** | **82%** | âœ… |

### è¦†ç›–ç‡æŠ¥å‘Š

```bash
# ç”ŸæˆHTMLæŠ¥å‘Š
pytest --cov=src --cov-report=html
# æ‰“å¼€ htmlcov/index.html

# ç”ŸæˆXMLæŠ¥å‘Šï¼ˆç”¨äºCIï¼‰
pytest --cov=src --cov-report=xml
```

## ğŸ”„ æŒç»­é›†æˆ

### GitHub Actionsé…ç½®

```yaml
# .github/workflows/test.yml
name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10]
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run tests
      run: |
        pytest --cov=src --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v2
```

### æµ‹è¯•ç¯å¢ƒçŸ©é˜µ

| Python | OS | çŠ¶æ€ |
|--------|-----|------|
| 3.8 | Ubuntu | âœ… |
| 3.9 | Ubuntu | âœ… |
| 3.10 | Ubuntu | âœ… |
| 3.9 | Windows | âœ… |
| 3.9 | macOS | âœ… |

## ğŸ¯ æµ‹è¯•æœ€ä½³å®è·µ

### 1. æµ‹è¯•ç‹¬ç«‹æ€§

**Independent test specialist:**

**First independent test:**
1. Create new MyClass instance
2. Call obj.method()
3. Assert result equals expected value

**Second independent test:**
1. Create fresh MyClass instance (new instance, not shared)
2. Call obj.other_method()
3. Assert result equals expected value

**Key principle**: Each test creates its own objects to ensure complete isolation

### 2. æ˜ç¡®çš„æ–­è¨€

**Clear assertion specialist:**

**Good practices:**
â€¢ Assert specific values: response.status_code == 200
â€¢ Assert absence of errors: "error" not in response.json()

**Avoid:**
â€¢ Vague assertions like "assert response" (å¤ªæ¨¡ç³Š)
â€¢ Use specific, meaningful assertions that clearly indicate what is being tested

### 3. æµ‹è¯•æ•°æ®ç®¡ç†

**Test data fixture specialist:**
1. Create sample_bible fixture:
   â€¢ Add @pytest.fixture decorator
   â€¢ Return dictionary with test data:
     - "title": "æµ‹è¯•å°è¯´"
     - "genre": "ç§‘å¹»"
     - "characters": ["A", "B", "C"]

**Data-driven test specialist:**
1. Accept sample_bible fixture as parameter
2. Call process_bible() with sample_bible data
3. Assert result is not None

### 4. å‚æ•°åŒ–æµ‹è¯•

**Parametrized test specialist:**
1. Add @pytest.mark.parametrize decorator with:
   â€¢ Parameter names: "input,expected"
   â€¢ Test cases list:
     - (1, "one")
     - (2, "two")
     - (3, "three")
2. Create test function accepting input and expected parameters
3. Assert number_to_word(input) equals expected value

**Result**: This will run 3 separate test cases automatically

## ğŸ› è°ƒè¯•æµ‹è¯•

### ä½¿ç”¨pdbè°ƒè¯•

**Debug test specialist:**
1. Create test function for debugging
2. Add debugging breakpoint:
   â€¢ Import pdb module
   â€¢ Call pdb.set_trace() to start debugger
3. Execute the function under test
4. Assert result equals expected value

**Usage**: Test will pause at breakpoint for interactive debugging

### æŸ¥çœ‹è¯¦ç»†å¤±è´¥ä¿¡æ¯

```bash
# æ˜¾ç¤ºå®Œæ•´çš„æ–­è¨€å¤±è´¥ä¿¡æ¯
pytest --tb=long

# æ˜¾ç¤ºæœ¬åœ°å˜é‡
pytest --showlocals

# å¤±è´¥æ—¶è¿›å…¥pdb
pytest --pdb
```

## ğŸ“Š æ€§èƒ½æµ‹è¯•

### åŸºå‡†æµ‹è¯•

**Performance benchmark specialist:**
1. Import pytest module
2. Create benchmark test:
   â€¢ Add @pytest.mark.benchmark decorator
   â€¢ Accept benchmark fixture parameter
3. Execute benchmarked function:
   â€¢ Call benchmark(function_to_test, arg1, arg2)
   â€¢ Store result
4. Assert result equals expected value

**Purpose**: Measures function execution time and performance

### è¶…æ—¶æµ‹è¯•

**Timeout test specialist:**
1. Add timeout decorator:
   â€¢ Use @pytest.mark.timeout(5) for 5-second timeout
2. Create test function
3. Execute potentially slow function:
   â€¢ Call slow_function()
   â€¢ Store result
4. Assert result is not None

**Behavior**: Test will fail if execution takes longer than specified timeout

## ğŸ” æµ‹è¯•æ£€æŸ¥æ¸…å•

### PRæäº¤å‰æ£€æŸ¥

- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡ `pytest`
- [ ] è¦†ç›–ç‡è¾¾æ ‡ `pytest --cov=src --cov-fail-under=80`
- [ ] æ— linté”™è¯¯ `flake8 src tests`
- [ ] ç±»å‹æ£€æŸ¥é€šè¿‡ `mypy src`
- [ ] æ–‡æ¡£æ›´æ–°
- [ ] æ–°åŠŸèƒ½æœ‰æµ‹è¯•
- [ ] æµ‹è¯•æœ‰æ„ä¹‰çš„åœºæ™¯

## ğŸ†˜ å¸¸è§é—®é¢˜

### Q: æµ‹è¯•è¿è¡Œå¾ˆæ…¢ï¼Ÿ
```bash
# å¹¶è¡Œè¿è¡Œ
pytest -n auto

# åªè¿è¡Œå¿«é€Ÿæµ‹è¯•
pytest -m "not slow"

# ä½¿ç”¨ç¼“å­˜
pytest --cache-show
```

### Q: Mockä¸ç”Ÿæ•ˆï¼Ÿ

**Mock path troubleshooting specialist:**
â€¢ Ensure correct patch path format
â€¢ Use full module path: 'src.module.function'
â€¢ Avoid shortened path: 'module.function' (incorrect)
â€¢ Verify the import path matches where the function is actually used

### Q: å¼‚æ­¥æµ‹è¯•å¤±è´¥ï¼Ÿ

**Async test troubleshooting specialist:**
1. Install pytest-asyncio plugin
2. Create async test function:
   â€¢ Add @pytest.mark.asyncio decorator
   â€¢ Define async function with "test_" prefix
3. Properly await async operations:
   â€¢ Use "await" keyword before async_function() call

**Common issue**: Missing @pytest.mark.asyncio decorator or not awaiting async calls

---

*æœ€åæ›´æ–°: 2025-01-30*