"""
Incremental Sync Mechanism
å¢é‡åŒæ­¥æœºåˆ¶
åªåŒæ­¥å˜åŒ–çš„éƒ¨åˆ†ï¼Œæé«˜æ•ˆç‡
"""

import hashlib
import json
import time
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
import difflib
import logging

logger = logging.getLogger(__name__)


@dataclass
class SyncRecord:
    """åŒæ­¥è®°å½•"""
    sync_id: str
    source_path: str
    target: str  # GitHub Issue IDæˆ–å…¶ä»–ç›®æ ‡
    timestamp: datetime
    content_hash: str
    changes: Dict[str, Any]
    sync_type: str  # full, incremental, patch
    success: bool
    error_message: Optional[str] = None
    metrics: Dict[str, int] = field(default_factory=dict)


@dataclass 
class ContentVersion:
    """å†…å®¹ç‰ˆæœ¬"""
    version: int
    content_hash: str
    timestamp: datetime
    size: int
    changes_from_previous: Optional[str] = None
    metadata: Dict = field(default_factory=dict)


class IncrementalSyncManager:
    """
    å¢é‡åŒæ­¥ç®¡ç†å™¨
    å®ç°é«˜æ•ˆçš„å†…å®¹åŒæ­¥
    """
    
    def __init__(self, cache_dir: str = "data/sync_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åŒæ­¥å†å²
        self.sync_history: List[SyncRecord] = []
        self.content_cache: Dict[str, str] = {}  # è·¯å¾„ -> å†…å®¹å“ˆå¸Œ
        self.version_history: Dict[str, List[ContentVersion]] = {}  # è·¯å¾„ -> ç‰ˆæœ¬å†å²
        
        # åŒæ­¥æ ‡è®°
        self.sync_markers: Dict[str, datetime] = {}  # è·¯å¾„ -> æœ€ååŒæ­¥æ—¶é—´
        
        # åŠ è½½ç¼“å­˜
        self._load_cache()
    
    def _load_cache(self):
        """åŠ è½½åŒæ­¥ç¼“å­˜"""
        # åŠ è½½åŒæ­¥æ ‡è®°
        markers_file = self.cache_dir / "sync_markers.json"
        if markers_file.exists():
            try:
                with open(markers_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.sync_markers = {
                        k: datetime.fromisoformat(v)
                        for k, v in data.items()
                    }
            except Exception as e:
                logger.error(f"åŠ è½½åŒæ­¥æ ‡è®°å¤±è´¥: {e}")
        
        # åŠ è½½å†…å®¹ç¼“å­˜
        cache_file = self.cache_dir / "content_cache.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.content_cache = json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½å†…å®¹ç¼“å­˜å¤±è´¥: {e}")
    
    def _compute_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    def _get_content(self, file_path: Path) -> Optional[str]:
        """è·å–æ–‡ä»¶å†…å®¹"""
        if not file_path.exists():
            return None
        
        try:
            if file_path.suffix == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.dumps(json.load(f), ensure_ascii=False, sort_keys=True)
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    # ============ å˜åŒ–æ£€æµ‹ ============
    
    def detect_changes(self, file_path: Path) -> Tuple[bool, Optional[Dict]]:
        """
        æ£€æµ‹æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
        è¿”å›: (æ˜¯å¦æœ‰å˜åŒ–, å˜åŒ–è¯¦æƒ…)
        """
        str_path = str(file_path)
        current_content = self._get_content(file_path)
        
        if current_content is None:
            return False, None
        
        current_hash = self._compute_hash(current_content)
        
        # æ£€æŸ¥ç¼“å­˜
        if str_path in self.content_cache:
            cached_hash = self.content_cache[str_path]
            if cached_hash == current_hash:
                return False, None  # æ— å˜åŒ–
            
            # è®¡ç®—å·®å¼‚
            changes = self._calculate_diff(str_path, current_content)
            
            # æ›´æ–°ç¼“å­˜
            self.content_cache[str_path] = current_hash
            
            return True, changes
        else:
            # æ–°æ–‡ä»¶
            self.content_cache[str_path] = current_hash
            return True, {"type": "new", "size": len(current_content)}
    
    def _calculate_diff(self, file_path: str, new_content: str) -> Dict:
        """
        è®¡ç®—å†…å®¹å·®å¼‚
        """
        # è·å–æ—§å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä»ç‰ˆæœ¬å†å²è·å–ï¼‰
        old_content = self._get_cached_content(file_path)
        
        if not old_content:
            return {"type": "new", "size": len(new_content)}
        
        # è®¡ç®—å·®å¼‚
        differ = difflib.unified_diff(
            old_content.splitlines(keepends=True),
            new_content.splitlines(keepends=True),
            lineterm=''
        )
        
        diff_lines = list(differ)
        
        # ç»Ÿè®¡å˜åŒ–
        added_lines = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))
        removed_lines = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))
        
        return {
            "type": "modified",
            "added_lines": added_lines,
            "removed_lines": removed_lines,
            "total_changes": added_lines + removed_lines,
            "diff_size": len(''.join(diff_lines))
        }
    
    def _get_cached_content(self, file_path: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„å†…å®¹"""
        cache_file = self.cache_dir / f"{hashlib.md5(file_path.encode()).hexdigest()}.cache"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return f.read()
            except:
                pass
        return None
    
    # ============ å¢é‡åŒæ­¥ ============
    
    def sync_incremental(self, source_path: Path, target: str,
                        force: bool = False) -> Tuple[bool, SyncRecord]:
        """
        æ‰§è¡Œå¢é‡åŒæ­¥
        """
        str_path = str(source_path)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åŒæ­¥
        if not force:
            last_sync = self.sync_markers.get(str_path)
            if last_sync:
                # å¦‚æœæœ€è¿‘åŒæ­¥è¿‡ï¼ˆæ¯”å¦‚5åˆ†é’Ÿå†…ï¼‰ï¼Œè·³è¿‡
                if datetime.now() - last_sync < timedelta(minutes=5):
                    logger.info(f"è·³è¿‡åŒæ­¥ {str_path} (æœ€è¿‘å·²åŒæ­¥)")
                    return False, None
        
        # æ£€æµ‹å˜åŒ–
        has_changes, changes = self.detect_changes(source_path)
        
        if not has_changes and not force:
            logger.info(f"æ— å˜åŒ–ï¼Œè·³è¿‡åŒæ­¥ {str_path}")
            return False, None
        
        # å‡†å¤‡åŒæ­¥å†…å®¹
        content = self._get_content(source_path)
        if not content:
            return False, None
        
        # å†³å®šåŒæ­¥ç±»å‹
        sync_type = self._determine_sync_type(changes)
        
        # æ‰§è¡ŒåŒæ­¥
        start_time = time.time()
        
        if sync_type == "incremental":
            sync_content = self._prepare_incremental_content(str_path, content, changes)
        elif sync_type == "patch":
            sync_content = self._prepare_patch_content(str_path, content, changes)
        else:
            sync_content = content
        
        # æ¨¡æ‹ŸåŒæ­¥åˆ°ç›®æ ‡ï¼ˆå®é™…åº”è¯¥è°ƒç”¨GitHub APIç­‰ï¼‰
        success = self._perform_sync(target, sync_content, sync_type)
        
        sync_time = time.time() - start_time
        
        # åˆ›å»ºåŒæ­¥è®°å½•
        sync_record = SyncRecord(
            sync_id=f"sync_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            source_path=str_path,
            target=target,
            timestamp=datetime.now(),
            content_hash=self._compute_hash(content),
            changes=changes or {},
            sync_type=sync_type,
            success=success,
            metrics={
                "sync_time_ms": int(sync_time * 1000),
                "content_size": len(content),
                "sync_size": len(sync_content)
            }
        )
        
        # æ›´æ–°è®°å½•
        if success:
            self.sync_markers[str_path] = datetime.now()
            self._cache_content(str_path, content)
            self._add_version(str_path, content, changes)
        
        self.sync_history.append(sync_record)
        self._save_cache()
        
        logger.info(f"åŒæ­¥å®Œæˆ: {str_path} -> {target} ({sync_type})")
        return success, sync_record
    
    def _determine_sync_type(self, changes: Optional[Dict]) -> str:
        """
        å†³å®šåŒæ­¥ç±»å‹
        """
        if not changes:
            return "full"
        
        if changes.get("type") == "new":
            return "full"
        
        total_changes = changes.get("total_changes", 0)
        
        if total_changes < 10:
            return "patch"  # å°æ”¹åŠ¨ç”¨è¡¥ä¸
        elif total_changes < 100:
            return "incremental"  # ä¸­ç­‰æ”¹åŠ¨ç”¨å¢é‡
        else:
            return "full"  # å¤§æ”¹åŠ¨å…¨é‡åŒæ­¥
    
    def _prepare_incremental_content(self, file_path: str, 
                                    content: str, changes: Dict) -> str:
        """
        å‡†å¤‡å¢é‡åŒæ­¥å†…å®¹
        åªåŒ…å«å˜åŒ–çš„éƒ¨åˆ†
        """
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åªå‘é€å˜åŒ–çš„éƒ¨åˆ†
        incremental = {
            "type": "incremental",
            "file_path": file_path,
            "timestamp": datetime.now().isoformat(),
            "changes": changes,
            "content_hash": self._compute_hash(content),
            "partial_content": content[:1000] if len(content) > 1000 else content
        }
        
        return json.dumps(incremental, ensure_ascii=False)
    
    def _prepare_patch_content(self, file_path: str,
                             content: str, changes: Dict) -> str:
        """
        å‡†å¤‡è¡¥ä¸å†…å®¹
        ä½¿ç”¨diffæ ¼å¼
        """
        old_content = self._get_cached_content(file_path) or ""
        
        # ç”Ÿæˆè¡¥ä¸
        diff = difflib.unified_diff(
            old_content.splitlines(keepends=True),
            content.splitlines(keepends=True),
            fromfile=f"{file_path}.old",
            tofile=file_path,
            lineterm=''
        )
        
        patch = {
            "type": "patch",
            "file_path": file_path,
            "timestamp": datetime.now().isoformat(),
            "patch": ''.join(diff)
        }
        
        return json.dumps(patch, ensure_ascii=False)
    
    def _perform_sync(self, target: str, content: str, sync_type: str) -> bool:
        """
        æ‰§è¡Œå®é™…çš„åŒæ­¥æ“ä½œ
        """
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„åŒæ­¥APIï¼ˆå¦‚GitHub APIï¼‰
        # ç°åœ¨åªæ˜¯æ¨¡æ‹Ÿ
        logger.info(f"åŒæ­¥åˆ° {target}: {sync_type} ({len(content)} bytes)")
        return True
    
    def _cache_content(self, file_path: str, content: str):
        """ç¼“å­˜å†…å®¹"""
        cache_file = self.cache_dir / f"{hashlib.md5(file_path.encode()).hexdigest()}.cache"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                f.write(content)
        except Exception as e:
            logger.error(f"ç¼“å­˜å†…å®¹å¤±è´¥: {e}")
    
    def _add_version(self, file_path: str, content: str, changes: Optional[Dict]):
        """æ·»åŠ ç‰ˆæœ¬è®°å½•"""
        if file_path not in self.version_history:
            self.version_history[file_path] = []
        
        versions = self.version_history[file_path]
        version_num = len(versions) + 1
        
        version = ContentVersion(
            version=version_num,
            content_hash=self._compute_hash(content),
            timestamp=datetime.now(),
            size=len(content),
            changes_from_previous=json.dumps(changes) if changes else None
        )
        
        versions.append(version)
        
        # åªä¿ç•™æœ€è¿‘10ä¸ªç‰ˆæœ¬
        if len(versions) > 10:
            versions.pop(0)
    
    # ============ æ‰¹é‡åŒæ­¥ ============
    
    def sync_directory(self, directory: Path, target_prefix: str,
                      pattern: str = "*.md") -> List[SyncRecord]:
        """
        åŒæ­¥æ•´ä¸ªç›®å½•
        """
        results = []
        
        for file_path in directory.glob(pattern):
            if file_path.is_file():
                target = f"{target_prefix}/{file_path.name}"
                success, record = self.sync_incremental(file_path, target)
                if record:
                    results.append(record)
        
        logger.info(f"æ‰¹é‡åŒæ­¥å®Œæˆ: {len(results)}ä¸ªæ–‡ä»¶")
        return results
    
    def get_pending_syncs(self, directory: Path, 
                         pattern: str = "*.md") -> List[Path]:
        """
        è·å–å¾…åŒæ­¥çš„æ–‡ä»¶
        """
        pending = []
        
        for file_path in directory.glob(pattern):
            if file_path.is_file():
                has_changes, _ = self.detect_changes(file_path)
                if has_changes:
                    pending.append(file_path)
        
        return pending
    
    # ============ åŒæ­¥ä¼˜åŒ– ============
    
    def optimize_sync_batch(self, files: List[Path]) -> List[List[Path]]:
        """
        ä¼˜åŒ–åŒæ­¥æ‰¹æ¬¡
        å°†æ–‡ä»¶åˆ†ç»„ä»¥æé«˜æ•ˆç‡
        """
        # æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
        small_files = []  # < 1KB
        medium_files = []  # 1KB - 100KB
        large_files = []   # > 100KB
        
        for file_path in files:
            if file_path.exists():
                size = file_path.stat().st_size
                if size < 1024:
                    small_files.append(file_path)
                elif size < 102400:
                    medium_files.append(file_path)
                else:
                    large_files.append(file_path)
        
        batches = []
        
        # å°æ–‡ä»¶å¯ä»¥æ‰¹é‡åŒæ­¥
        if small_files:
            # æ¯æ‰¹æœ€å¤š20ä¸ªå°æ–‡ä»¶
            for i in range(0, len(small_files), 20):
                batches.append(small_files[i:i+20])
        
        # ä¸­ç­‰æ–‡ä»¶é€‚åº¦æ‰¹é‡
        if medium_files:
            # æ¯æ‰¹æœ€å¤š5ä¸ªä¸­ç­‰æ–‡ä»¶
            for i in range(0, len(medium_files), 5):
                batches.append(medium_files[i:i+5])
        
        # å¤§æ–‡ä»¶å•ç‹¬åŒæ­¥
        for large_file in large_files:
            batches.append([large_file])
        
        return batches
    
    # ============ æŠ¥å‘Šç”Ÿæˆ ============
    
    def generate_sync_report(self) -> str:
        """
        ç”ŸæˆåŒæ­¥æŠ¥å‘Š
        """
        report = []
        report.append("# ğŸ“Š å¢é‡åŒæ­¥æŠ¥å‘Š")
        report.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_syncs = len(self.sync_history)
        successful_syncs = sum(1 for s in self.sync_history if s.success)
        
        report.append("## ğŸ“ˆ åŒæ­¥ç»Ÿè®¡")
        report.append(f"- æ€»åŒæ­¥æ¬¡æ•°: {total_syncs}")
        report.append(f"- æˆåŠŸç‡: {successful_syncs}/{total_syncs}")
        
        if self.sync_history:
            # åŒæ­¥ç±»å‹åˆ†å¸ƒ
            type_counts = {}
            for record in self.sync_history:
                sync_type = record.sync_type
                type_counts[sync_type] = type_counts.get(sync_type, 0) + 1
            
            report.append("")
            report.append("## ğŸ”„ åŒæ­¥ç±»å‹åˆ†å¸ƒ")
            for sync_type, count in sorted(type_counts.items()):
                percentage = (count / total_syncs) * 100
                report.append(f"- {sync_type}: {count}æ¬¡ ({percentage:.1f}%)")
            
            # æ€§èƒ½æŒ‡æ ‡
            sync_times = [r.metrics.get("sync_time_ms", 0) for r in self.sync_history]
            if sync_times:
                avg_time = sum(sync_times) / len(sync_times)
                report.append("")
                report.append("## âš¡ æ€§èƒ½æŒ‡æ ‡")
                report.append(f"- å¹³å‡åŒæ­¥æ—¶é—´: {avg_time:.0f}ms")
                report.append(f"- æœ€å¿«: {min(sync_times)}ms")
                report.append(f"- æœ€æ…¢: {max(sync_times)}ms")
            
            # æ•°æ®é‡ç»Ÿè®¡
            total_synced = sum(r.metrics.get("sync_size", 0) for r in self.sync_history)
            total_original = sum(r.metrics.get("content_size", 0) for r in self.sync_history)
            
            if total_original > 0:
                compression = (1 - total_synced / total_original) * 100
                report.append("")
                report.append("## ğŸ’¾ æ•°æ®ä¼˜åŒ–")
                report.append(f"- åŸå§‹æ•°æ®: {total_original/1024:.1f}KB")
                report.append(f"- åŒæ­¥æ•°æ®: {total_synced/1024:.1f}KB")
                report.append(f"- èŠ‚çœ: {compression:.1f}%")
        
        # æœ€è¿‘åŒæ­¥
        if self.sync_history:
            report.append("")
            report.append("## ğŸ• æœ€è¿‘åŒæ­¥")
            for record in self.sync_history[-5:]:
                time_str = record.timestamp.strftime("%H:%M:%S")
                report.append(f"- {time_str}: {Path(record.source_path).name} ({record.sync_type})")
        
        # å¾…åŒæ­¥æ–‡ä»¶
        report.append("")
        report.append("## â³ å¾…åŒæ­¥çŠ¶æ€")
        report.append(f"- ç¼“å­˜æ–‡ä»¶æ•°: {len(self.content_cache)}")
        report.append(f"- ç›‘æ§è·¯å¾„æ•°: {len(self.sync_markers)}")
        
        return "\n".join(report)
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        # ä¿å­˜åŒæ­¥æ ‡è®°
        markers_data = {
            k: v.isoformat()
            for k, v in self.sync_markers.items()
        }
        
        markers_file = self.cache_dir / "sync_markers.json"
        with open(markers_file, 'w', encoding='utf-8') as f:
            json.dump(markers_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å†…å®¹ç¼“å­˜
        cache_file = self.cache_dir / "content_cache.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.content_cache, f, ensure_ascii=False, indent=2)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºåŒæ­¥ç®¡ç†å™¨
    sync_manager = IncrementalSyncManager()
    
    # æ¨¡æ‹Ÿæ–‡ä»¶è·¯å¾„
    test_dir = Path("data/chapters")
    test_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_file = test_dir / "chapter1.md"
    test_file.write_text("# ç¬¬ä¸€ç« \n\nè¿™æ˜¯ç¬¬ä¸€ç« çš„å†…å®¹...")
    
    # ç¬¬ä¸€æ¬¡åŒæ­¥ï¼ˆå…¨é‡ï¼‰
    success, record = sync_manager.sync_incremental(
        test_file,
        "github://issue/1"
    )
    print(f"ç¬¬ä¸€æ¬¡åŒæ­¥: {record.sync_type if record else 'skipped'}")
    
    # ä¿®æ”¹æ–‡ä»¶
    test_file.write_text("# ç¬¬ä¸€ç« \n\nè¿™æ˜¯ç¬¬ä¸€ç« çš„å†…å®¹...\n\næ–°å¢äº†ä¸€æ®µå†…å®¹ã€‚")
    
    # ç¬¬äºŒæ¬¡åŒæ­¥ï¼ˆå¢é‡ï¼‰
    success, record = sync_manager.sync_incremental(
        test_file,
        "github://issue/1"
    )
    print(f"ç¬¬äºŒæ¬¡åŒæ­¥: {record.sync_type if record else 'skipped'}")
    
    # æ‰¹é‡åŒæ­¥
    for i in range(2, 5):
        chapter_file = test_dir / f"chapter{i}.md"
        chapter_file.write_text(f"# ç¬¬{i}ç« \n\nå†…å®¹...")
    
    records = sync_manager.sync_directory(test_dir, "github://issues")
    print(f"æ‰¹é‡åŒæ­¥: {len(records)}ä¸ªæ–‡ä»¶")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = sync_manager.generate_sync_report()
    print("\n" + report)